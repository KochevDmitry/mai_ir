import requests
import os
import time
import threading
from datetime import datetime, timedelta
import logging
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import re
import sys
import signal

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('f1news_parser.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


class StrictRateLimiter:

    def __init__(self, rate=1.0):

        self.rate = rate
        self.min_interval = 1.0 / rate  
        self.last_request_time = 0
        self.lock = threading.Lock()
        logger.info(f"Rate limiter –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω: {rate} –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É")

    def wait(self):
        with self.lock:
            current_time = time.time()

            if self.last_request_time > 0:
                elapsed = current_time - self.last_request_time

                if elapsed < self.min_interval:
                    sleep_time = self.min_interval - elapsed
                    logger.debug(f"Rate limiting: –æ–∂–∏–¥–∞–Ω–∏–µ {sleep_time:.3f} —Å–µ–∫")
                    time.sleep(sleep_time)
                    current_time = time.time() 

            self.last_request_time = current_time

    def get_stats(self):
        with self.lock:
            return {
                'rate': self.rate,
                'min_interval': self.min_interval,
                'last_request_time': self.last_request_time,
                'time_since_last': time.time() - self.last_request_time if self.last_request_time > 0 else 0
            }


class ProgressMonitor:

    def __init__(self, parser_instance):
        self.parser = parser_instance
        self.running = True
        self.thread = None
        self.start_time = time.time()

    def display_progress(self):
        while self.running:
            try:
                count = self.parser.articles_count
                target = self.parser.target_count
                percentage = (count / target * 100) if target > 0 else 0

                elapsed = time.time() - self.start_time
                if count > 0:
                    articles_per_hour = (count / elapsed) * 3600
                    estimated_total = (target - count) / (count / elapsed) if count > 0 else 0
                    eta_str = str(timedelta(seconds=int(estimated_total))) if estimated_total > 0 else "N/A"
                else:
                    articles_per_hour = 0
                    eta_str = "N/A"

                sys.stdout.write(f"\r–ü—Ä–æ–≥—Ä–µ—Å—Å: {count}/{target} —Å—Ç–∞—Ç–µ–π ({percentage:.2f}%) | "
                                 f"–°–∫–æ—Ä–æ—Å—Ç—å: {articles_per_hour:.1f}/—á–∞—Å | "
                                 f"ETA: {eta_str} | "
                                 f"–î–∞—Ç–∞: {self.parser.current_date_str if hasattr(self.parser, 'current_date_str') else 'N/A'}")
                sys.stdout.flush()

                if count >= target:
                    print(f"\n\nüéâ –¶–µ–ª—å –¥–æ—Å—Ç–∏–≥–Ω—É—Ç–∞! –°–æ–±—Ä–∞–Ω–æ {count} —Å—Ç–∞—Ç–µ–π.")
                    self.running = False
                    break

                time.sleep(1)  
            except KeyboardInterrupt:
                break
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –≤ –º–æ–Ω–∏—Ç–æ—Ä–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
                time.sleep(5)

    def start(self):
        self.thread = threading.Thread(target=self.display_progress, daemon=True)
        self.thread.start()

    def stop(self):
        self.running = False
        if self.thread:
            self.thread.join(timeout=2)
        print()  

class F1NewsParser:
    def __init__(self, base_url="https://www.f1news.ru"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'F1NewsParser/1.0 (educational project; contact@example.com)',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate, br',
        })

        self.rate_limiter = StrictRateLimiter(rate=2.0) 

        self.pages_dir = "pages"
        os.makedirs(self.pages_dir, exist_ok=True)

        self.articles_count = 0
        self.target_count = 30000
        self.delay = 1  
        self.current_date_str = ""

        self.total_requests = 0
        self.successful_requests = 0
        self.failed_requests = 0

        self.running = True
        self.monitor = ProgressMonitor(self)

        signal.signal(signal.SIGINT, self.signal_handler)
        signal.signal(signal.SIGTERM, self.signal_handler)

        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–∞—Ä—Å–µ—Ä. –°—Ç–∞—Ç—å–∏ –±—É–¥—É—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {os.path.abspath(self.pages_dir)}")

    def signal_handler(self, signum, frame):
        """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å–∏–≥–Ω–∞–ª–æ–≤ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
        logger.info(f"\n–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª {signum}. –ó–∞–≤–µ—Ä—à–∞–µ–º —Ä–∞–±–æ—Ç—É...")
        self.running = False
        self.monitor.stop()
        self.print_statistics()
        sys.exit(0)

    def make_rate_limited_request(self, url, method='GET', timeout=10, **kwargs):
        if not self.running:
            return None

        self.total_requests += 1

        try:
            self.rate_limiter.wait()

            logger.debug(f"–ó–∞–ø—Ä–æ—Å #{self.total_requests}: {url[:80]}...")

            response = self.session.request(method, url, timeout=timeout, **kwargs)

            if response.status_code == 200:
                self.successful_requests += 1
            else:
                self.failed_requests += 1
                logger.warning(f"HTTP {response.status_code} –¥–ª—è {url}")

            return response

        except requests.exceptions.Timeout:
            self.failed_requests += 1
            logger.warning(f"–¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞: {url}")
            return None
        except requests.exceptions.RequestException as e:
            self.failed_requests += 1
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ {url}: {e}")
            return None
        except Exception as e:
            self.failed_requests += 1
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ {url}: {e}")
            return None

    def parse_date_articles(self, date_str):
        """–ü–∞—Ä—Å–∏—Ç –≤—Å–µ —Å—Ç–∞—Ç—å–∏ –∑–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—É—é –¥–∞—Ç—É"""
        url = f"{self.base_url}/news/{date_str}/"

        try:
            logger.debug(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –¥–∞—Ç—ã: {url}")
            response = self.make_rate_limited_request(url, timeout=15)

            if response is None:
                return []

            if response.status_code == 404:
                logger.debug(f"–°—Ç—Ä–∞–Ω–∏—Ü–∞ {url} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ (404)")
                return []

            soup = BeautifulSoup(response.content, 'html.parser')

            articles = set()

            all_links = soup.find_all('a', href=True)
            for link in all_links:
                href = link['href']
                if '/news/' in href and date_str.replace('/', '/') in href:
                    full_url = urljoin(self.base_url, href)
                    articles.add(full_url)

            news_items = soup.find_all(['div', 'article', 'li'], class_=re.compile(r'news|article|item', re.I))
            for item in news_items:
                links = item.find_all('a', href=True)
                for link in links:
                    href = link['href']
                    if '/news/' in href:
                        full_url = urljoin(self.base_url, href)
                        articles.add(full_url)

            articles_list = list(articles)
            logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(articles_list)} —Å—Ç–∞—Ç–µ–π –∑–∞ {date_str}")

            return articles_list

        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –ø–∞—Ä—Å–∏–Ω–≥–µ {url}: {e}")
            return []

    def save_article(self, url):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å—Ç–∞—Ç—å–∏"""
        if not self.running:
            return False

        try:
            if self.articles_count >= self.target_count:
                return False

            logger.debug(f"–ó–∞–≥—Ä—É–∑–∫–∞ —Å—Ç–∞—Ç—å–∏: {url}")

            response = self.make_rate_limited_request(url, timeout=20)

            if response is None or response.status_code != 200:
                return False

           
            url_path = url.replace(self.base_url, '').strip('/')
            if not url_path:
                url_path = f"article_{self.articles_count + 1}"

            filename = url_path.replace('/', '_')
            if not filename.endswith('.html'):
                filename += '.html'

            filename = re.sub(r'[<>:"\\|?*]', '_', filename)

            if len(filename) > 100:
                name, ext = os.path.splitext(filename)
                filename = name[:95] + ext

            filepath = os.path.join(self.pages_dir, filename)

            if os.path.exists(filepath):
                logger.debug(f"–°—Ç–∞—Ç—å—è —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {filename}")
                self.articles_count += 1
                return True

            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(response.text)

            self.articles_count += 1

            self.save_metadata(url, filename)

            logger.info(f"‚úì –°–æ—Ö—Ä–∞–Ω–µ–Ω–∞ —Å—Ç–∞—Ç—å—è #{self.articles_count}: {filename}")

            return True

        except Exception as e:
            logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ {url}: {e}")
            return False

    def save_metadata(self, url, filename):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –≤ –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª"""
        metadata_file = os.path.join(self.pages_dir, "metadata.csv")

        try:
            if not os.path.exists(metadata_file):
                with open(metadata_file, 'w', encoding='utf-8') as f:
                    f.write("id,url,filename,date_downloaded,date_published\n")

            date_published = "unknown"
            match = re.search(r'news/(\d{4}/\d{2}/\d{2})/', url)
            if match:
                date_published = match.group(1)

            with open(metadata_file, 'a', encoding='utf-8') as f:
                f.write(f"{self.articles_count},{url},{filename},"
                        f"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')},{date_published}\n")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

    def date_to_str(self, date_obj):
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –æ–±—ä–µ–∫—Ç datetime –≤ —Å—Ç—Ä–æ–∫—É —Ñ–æ—Ä–º–∞—Ç–∞ YYYY/MM/DD"""
        return date_obj.strftime("%Y/%m/%d")

    def str_to_date(self, date_str):
        """–ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç —Å—Ç—Ä–æ–∫—É –≤ –æ–±—ä–µ–∫—Ç datetime"""
        try:
            return datetime.strptime(date_str, "%Y/%m/%d")
        except ValueError:
            for fmt in ["%Y-%m-%d", "%Y.%m.%d", "%d/%m/%Y", "%d.%m.%Y"]:
                try:
                    return datetime.strptime(date_str, fmt)
                except ValueError:
                    continue
            raise ValueError(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {date_str}")

    def get_existing_articles_count(self):
        if not os.path.exists(self.pages_dir):
            return 0

        html_files = [f for f in os.listdir(self.pages_dir) if f.endswith('.html')]
        return len(html_files)

    def get_last_processed_date(self):
        metadata_file = os.path.join(self.pages_dir, "metadata.csv")

        if not os.path.exists(metadata_file):
            return None

        try:
            with open(metadata_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                if len(lines) > 1:  
                    last_line = lines[-1].strip()
                    parts = last_line.split(',')
                    if len(parts) >= 5 and parts[4] != 'unknown' and parts[4] != 'date_published':
                        date_str = parts[4].replace('/', '-')
                        try:
                            return datetime.strptime(date_str, "%Y-%m-%d")
                        except:
                            return None
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö: {e}")

        return None

    def print_statistics(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ä–∞–±–æ—Ç—ã –ø–∞—Ä—Å–µ—Ä–∞"""
        success_rate = (self.successful_requests / self.total_requests * 100) if self.total_requests > 0 else 0

        print("\n" + "=" * 70)
        print("–°–¢–ê–¢–ò–°–¢–ò–ö–ê –†–ê–ë–û–¢–´ –ü–ê–†–°–ï–†–ê")
        print("=" * 70)
        print(f"–í—Å–µ–≥–æ —Å—Ç–∞—Ç–µ–π —Å–æ–±—Ä–∞–Ω–æ: {self.articles_count}")
        print(f"–¶–µ–ª–µ–≤–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ: {self.target_count}")
        print(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {self.total_requests}")
        print(f"–£—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {self.successful_requests} ({success_rate:.1f}%)")
        print(f"–ù–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {self.failed_requests}")

        stats = self.rate_limiter.get_stats()
        print(f"\n–°–¢–ê–¢–ò–°–¢–ò–ö–ê RATE LIMITER:")
        print(f"  –ó–∞–ø—Ä–æ—Å–æ–≤ –≤ —Å–µ–∫—É–Ω–¥—É: {stats['rate']}")
        print(f"  –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –∏–Ω—Ç–µ—Ä–≤–∞–ª: {stats['min_interval']:.3f} —Å–µ–∫")
        print(f"  –í—Ä–µ–º—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞: {stats['time_since_last']:.3f} —Å–µ–∫")

        print("\n–°–û–í–ï–¢–´:")
        if success_rate < 90:
            print("   –ú–Ω–æ–≥–æ –Ω–µ—É–¥–∞—á–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç—É.")
        if self.articles_count < self.target_count * 0.1:
            print("   –°–æ–±—Ä–∞–Ω–æ –º–∞–ª–æ —Å—Ç–∞—Ç–µ–π. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Å—Ç–∞—Ä—Ç–æ–≤—É—é –¥–∞—Ç—É.")
        print("=" * 70)

    def run(self, start_date_str=None):

        existing_count = self.get_existing_articles_count()
        self.articles_count = existing_count

        if existing_count > 0:
            print(f"–ù–∞–π–¥–µ–Ω–æ {existing_count} —É–∂–µ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Å—Ç–∞—Ç–µ–π.")
            resume = input("–ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Å –º–µ—Å—Ç–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏? (y/n): ").strip().lower()

            if resume == 'y':
                last_date = self.get_last_processed_date()
                if last_date:
                    print(f"–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º —Å –¥–∞—Ç—ã: {last_date.strftime('%Y/%m/%d')}")
                    start_date = last_date
                else:
                    print("–ù–µ —É–¥–∞–ª–æ—Å—å –æ–ø—Ä–µ–¥–µ–ª–∏—Ç—å –ø–æ—Å–ª–µ–¥–Ω—é—é –¥–∞—Ç—É. –ù–∞—á–∏–Ω–∞–µ–º –∑–∞–Ω–æ–≤–æ.")
                    start_date_str = input("–í–≤–µ–¥–∏—Ç–µ —Å—Ç–∞—Ä—Ç–æ–≤—É—é –¥–∞—Ç—É (–ì–ì–ì–ì/–ú–ú/–î–î): ").strip()
                    if not start_date_str:
                        start_date_str = "2025/12/22"
                    start_date = self.str_to_date(start_date_str)
            else:
                start_date_str = input("–í–≤–µ–¥–∏—Ç–µ —Å—Ç–∞—Ä—Ç–æ–≤—É—é –¥–∞—Ç—É (–ì–ì–ì–ì/–ú–ú/–î–î): ").strip()
                if not start_date_str:
                    start_date_str = "2025/12/22"
                start_date = self.str_to_date(start_date_str)
        else:
            if not start_date_str:
                start_date_str = input("–í–≤–µ–¥–∏—Ç–µ —Å—Ç–∞—Ä—Ç–æ–≤—É—é –¥–∞—Ç—É (–ì–ì–ì–ì/–ú–ú/–î–î, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2025/12/22): ").strip()
                if not start_date_str:
                    start_date_str = "2025/12/22"

            try:
                start_date = self.str_to_date(start_date_str)
            except ValueError as e:
                print(f"–û—à–∏–±–∫–∞: {e}")
                start_date_str = "2025/12/22"
                start_date = self.str_to_date(start_date_str)

        logger.info("=" * 60)
        logger.info(f"–ù–ê–ß–ê–õ–û –†–ê–ë–û–¢–´ –ü–ê–†–°–ï–†–ê F1NEWS")
        logger.info(f"–¶–µ–ª—å: —Å–æ–±—Ä–∞—Ç—å {self.target_count} —Å—Ç–∞—Ç–µ–π")
        logger.info(f"–£–∂–µ —Å–æ–±—Ä–∞–Ω–æ: {existing_count} —Å—Ç–∞—Ç–µ–π")
        logger.info(f"–û—Å—Ç–∞–ª–æ—Å—å —Å–æ–±—Ä–∞—Ç—å: {self.target_count - existing_count} —Å—Ç–∞—Ç–µ–π")
        logger.info(f"–°—Ç–∞—Ä—Ç–æ–≤–∞—è –¥–∞—Ç–∞: {start_date.strftime('%Y/%m/%d')}")
        logger.info(f"Rate limiting: 1 –∑–∞–ø—Ä–æ—Å –≤ —Å–µ–∫—É–Ω–¥—É")
        logger.info("=" * 60)

        print(f"\n{'=' * 60}")
        print(f"–¶–ï–õ–¨: {self.target_count} —Å—Ç–∞—Ç–µ–π")
        print(f"–°–¢–ê–†–¢: {start_date.strftime('%Y/%m/%d')}")
        print(f"RATE LIMITING: 1 –∑–∞–ø—Ä–æ—Å –≤ —Å–µ–∫—É–Ω–¥—É")
        print(f"–ó–ê–î–ï–†–ñ–ö–ê –ú–ï–ñ–î–£ –î–ê–¢–ê–ú–ò: {self.delay} —Å–µ–∫")
        print(f"{'=' * 60}\n")

        self.monitor.start()

        current_date = start_date
        days_without_articles = 0
        consecutive_empty_days_limit = 30  
        try:
            while self.articles_count < self.target_count and self.running:
                self.current_date_str = self.date_to_str(current_date)

                if days_without_articles >= consecutive_empty_days_limit:
                    logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days_without_articles} –¥–Ω–µ–π. –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è.")
                    print(f"\n –ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –∑–∞ {days_without_articles} –¥–Ω–µ–π. –í–æ–∑–º–æ–∂–Ω–æ, –¥–æ—Å—Ç–∏–≥–ª–∏ –∫–æ–Ω—Ü–∞ –∞—Ä—Ö–∏–≤–∞.")
                    break

                articles = self.parse_date_articles(self.current_date_str)

                if articles:
                    days_without_articles = 0 
                    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(articles)} —Å—Ç–∞—Ç–µ–π –∑–∞ {self.current_date_str}")

                    for i, article_url in enumerate(articles, 1):
                        if not self.running or self.articles_count >= self.target_count:
                            break

                        logger.debug(f"–°—Ç–∞—Ç—å—è {i}/{len(articles)}: {article_url}")
                        self.save_article(article_url)


                else:
                    days_without_articles += 1
                    logger.debug(f"–°—Ç–∞—Ç–µ–π –∑–∞ {self.current_date_str} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")

                    if days_without_articles % 10 == 0:
                        logger.warning(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ —Å—Ç–∞—Ç–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days_without_articles} –¥–Ω–µ–π")

                current_date -= timedelta(days=1)

                time.sleep(self.delay)

                if self.articles_count % 100 == 0 and self.articles_count > 0:
                    logger.info(f"–ü—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–π –∏—Ç–æ–≥: {self.articles_count}/{self.target_count} —Å—Ç–∞—Ç–µ–π")
                    self.print_statistics()

            self.monitor.stop()
            self.print_statistics()

            if self.articles_count >= self.target_count:
                logger.info("=" * 60)
                logger.info(f" –¶–ï–õ–¨ –î–û–°–¢–ò–ì–ù–£–¢–ê!")
                logger.info(f"–í—Å–µ–≥–æ —Å–æ–±—Ä–∞–Ω–æ: {self.articles_count} —Å—Ç–∞—Ç–µ–π")
                logger.info(f"–°—Ç–∞—Ç—å–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {os.path.abspath(self.pages_dir)}")
                logger.info("=" * 60)

                print("\n" + "=" * 60)
                print(f"–£–°–ü–ï–• –°–æ–±—Ä–∞–Ω–æ {self.articles_count} —Å—Ç–∞—Ç–µ–π!")
                print(f" –ü–∞–ø–∫–∞ —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏: {os.path.abspath(self.pages_dir)}")
                print(f" –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ: {os.path.join(self.pages_dir, 'metadata.csv')}")
                print("=" * 60)
            else:
                logger.info(f"–†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ–±—Ä–∞–Ω–æ {self.articles_count} —Å—Ç–∞—Ç–µ–π")
                print(f"\n\n–†–∞–±–æ—Ç–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –°–æ–±—Ä–∞–Ω–æ {self.articles_count} —Å—Ç–∞—Ç–µ–π.")

        except KeyboardInterrupt:
            self.monitor.stop()
            self.print_statistics()
            logger.info("–ü–∞—Ä—Å–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
            print(f"\n\n‚èπ –ü–∞—Ä—Å–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –°–æ–±—Ä–∞–Ω–æ {self.articles_count} —Å—Ç–∞—Ç–µ–π.")
        except Exception as e:
            self.monitor.stop()
            self.print_statistics()
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)
            print(f"\n\n–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
            print(f"–°–æ–±—Ä–∞–Ω–æ {self.articles_count} —Å—Ç–∞—Ç–µ–π.")


def main():
    try:
        try:
            import requests
            import bs4
        except ImportError as e:
            print(f" –û—à–∏–±–∫–∞: –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏")
            return

        parser = F1NewsParser()

        if len(sys.argv) > 1:
            start_date = sys.argv[1]
            parser.run(start_date)
        else:
            parser.run()

    except Exception as e:
        print(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}")
        logger.error(f"–ù–µ–ø—Ä–µ–¥–≤–∏–¥–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞: {e}", exc_info=True)


if __name__ == "__main__":
    main()
