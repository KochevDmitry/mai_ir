\documentclass[a4paper,12pt]{article}

% --- Настройки языка и шрифтов ---
\usepackage[T2A]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[russian]{babel}

% --- Графика и таблицы ---
\usepackage{amsmath, amsfonts, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{geometry}
\usepackage{float}
\usepackage{csquotes}
\usepackage{xcolor}
\usepackage{realboxes} % Для заливки кода

\geometry{left=2cm, right=2cm, top=2cm, bottom=2cm}

% --- Команда для оформления кода в строке (подчеркивания теперь не ломают билд) ---
\definecolor{codebg}{rgb}{0.95,0.95,0.95}
\newcommand{\code}[1]{\Colorbox{codebg}{\texttt{\detokenize{#1}}}}

% --- Настройка листингов кода ---
\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\small\ttfamily,
    keywordstyle=\color{blue},
    stringstyle=\color{red},
    commentstyle=\color{green!60!black},
    numbers=left,
    numberstyle=\tiny,
    frame=single,
    breaklines=true,
    inputencoding=utf8,
    extendedchars=true,
    keepspaces=true
}

\usepackage{hyperref}

\begin{document}

% --- Титульный лист ---
\begin{titlepage}
\begin{center}
\bfseries
{\Large Московский авиационный институт\\ (национальный исследовательский университет)}
\vspace{48pt}

{\large Факультет компьютерных наук и прикладной математики}
\vspace{36pt}

{\large Кафедра вычислительной математики и~программирования}
\vspace{48pt}

Лабораторные работы по курсу \enquote{Информационный поиск}
\end{center}

\vspace{72pt}
\begin{flushright}
\begin{tabular}{rl}
Студент: & Д.\,О. Кочев \\
Преподаватель: & А.\,А. Кухтичев \\
Группа: & М8О-406Б-22 \\
Дата: & \\
Оценка: & \\
Подпись: & \\
\end{tabular}
\end{flushright}

\vfill
\begin{center}
\bfseries
Москва, \the\year
\end{center}
\end{titlepage}

\pagebreak
\tableofcontents
\newpage

% --- ЛР 1 ---
\section{Лабораторная работа №1. Анализ корпуса документов}

\subsection{Описание корпуса}
Для анализа использовались документы, собранные с двух новостных ресурсов:
\code{f1news.ru} и \code{f1report.ru}.
Корпус представлен HTML-файлами, содержащими новостные статьи, сохранённые
по датам публикации.

Оба сайта предоставляют встроенный поиск по новостям, что подтверждает пригодность корпуса. Однако существующие поисковые системы имеют ограниченные возможности: нельзя использовать сложные логические запросы, фильтры по рубрикам реализованы частично.

\subsection{Методика анализа}
Анализ корпуса выполнялся с использованием Python-скрипта,
осуществляющего параллельную обработку HTML-документов.
Для очистки HTML-разметки применялась библиотека \code{selectolax},
обеспечивающая высокую скорость парсинга.

В ходе обработки для каждого документа вычислялись:
\begin{itemize}
    \item размер исходного HTML-файла;
    \item размер очищенного текстового содержимого;
    \item средний размер текста одного документа.
\end{itemize}

Для ускорения обработки использовалась многопроцессорная обработка
с применением модуля \code{multiprocessing} и автоматическим
определением числа доступных ядер процессора.

\subsection{Результаты анализа}
Результаты статистического анализа корпуса представлены в таблице~\ref{tab:corpus_stats}.

\begin{table}[htbp]
\centering
\caption{Статистика корпуса документов}
\label{tab:corpus_stats}
\vspace{10pt}
\begin{tabular}{lrrrr}
\toprule
\textbf{Источник} & \textbf{Кол-во} & \textbf{Сырой, МБ} & \textbf{Текст, МБ} & \textbf{Ср. текст, КБ} \\
\midrule
f1news.ru   & 28634 & 2331.99 & 232.25 & 8.31 \\
f1report.ru & 7457  & 804.23  & 134.14 & 18.42 \\
\bottomrule
\end{tabular}
\end{table}

Общее время выполнения анализа корпуса составило \textbf{2398.1 секунды}.


\newpage

% --- ЛР 2 ---
\section{Лабораторная работа №2. Поисковый робот и сбор данных}

\subsection{Цель и архитектура робота}
Целью работы является разработка устойчивого поискового робота.
Архитектура системы включает:
\begin{itemize}
    \item \textbf{Библиотеки}: \code{requests}, \code{BeautifulSoup4}, \code{psycopg2}.
    \item \textbf{Хранилище}: Реляционная база данных PostgreSQL.
\end{itemize}
Робот принимает путь к конфигурационному файлу в формате YAML, который содержит данные для подключения к базе данных и настройки логики работы, включая задержку между запросами и другие параметры, необходимые для обхода страниц.

\subsection{Реализация и алгоритм работы}
Созданы два робота для двух разных источников, которые реализуют стратегию обхода архива. Основные этапы:
\begin{enumerate}
    \item Первый робот извлекает статьи на странице \code{f1news.ru/news/YYYY/MM/DD/}.
    \item В случае второго ресурса робот идет по подготовленной карте сайта.
    \item Получаение html кода и сохранение контента в таблицу \code{articles}.
\end{enumerate}
Робот использует задержку между запросами, настраиваемую через конфиг, чтобы избежать блокировки со стороны сайтов.

\subsection{Особенности устойчивости}
Для обеспечения надежной работы реализовано фиксирование даты таблицы и также использование хеширвания для контроля изменений.

\subsection{База данных}
Данные хранятся в PostgreSQL.

Таблица для хранения всех статей - \code{articles}:
\begin{lstlisting}
articles (
    id,
    url,
    html_content,
    source_name,
    crawl_timestamp,
    crawl_date,
    filename
)
\end{lstlisting}
\newpage

% --- ЛР 3-5 ---
\section{Лабораторная работа №3--5. Обработка текста}
\subsection{Токенизация и частотный анализ}

На данном этапе выполнялась токенизация текстового корпуса и подсчёт
частот вхождения слов с целью изучения статистических свойств текста
и проверки выполнения закона Ципфа.

\subsection{Подготовка данных}
Входными данными являлся очищенный текст корпуса, полученный на этапе
предварительной обработки HTML-документов.
Общий размер анализируемого файла составил \textbf{86 МБ},
при этом объём извлечённого текстового содержимого — \textbf{76.9 МБ}.

\subsection{Алгоритм токенизации}
Токенизация выполнялась с использованием хеш-таблицы для хранения
частот слов, что позволило добиться линейной сложности обработки текста.
В качестве токенов рассматривались последовательности символов,
соответствующие словам естественного языка.

Основные этапы алгоритма:
\begin{enumerate}
    \item последовательное чтение текстового файла;
    \item выделение токенов и приведение их к единому виду;
    \item добавление токенов в хеш-таблицу с подсчётом частот;
    \item сортировка уникальных токенов по убыванию частоты
    с использованием алгоритма QuickSort.
\end{enumerate}

Сложность алгоритма составила:
\[
O(n + m \log m),
\]
где $n$ — размер текста, а $m$ — количество уникальных токенов.

\subsection{Статистические характеристики корпуса}
В результате токенизации были получены следующие показатели:
\begin{enumerate}
    \item общее количество токенов — \textbf{13\,078\,112};
    \item количество уникальных токенов — \textbf{129\,367};
    \item коэффициент уникальности — \textbf{0.99\%};
    \item средняя длина токена — \textbf{5.28 символа}.
\end{enumerate}

Число hapax legomena (слов, встречающихся один раз)
составило \textbf{58\,007}, что соответствует \textbf{44.8\%}
от общего числа уникальных токенов.

\subsection{Наиболее частотные токены}
Анализ частот показал, что наиболее употребимыми словами в корпусе
являются служебные части речи русского языка.
Покрытие текста наиболее частотными токенами составило:
\begin{itemize}
    \item топ-10 токенов — \textbf{8.23\%} корпуса;
    \item топ-100 токенов — \textbf{18.11\%} корпуса.
\end{itemize}

\subsection{Анализ закона Ципфа}
Для оценки соответствия распределения слов закону Ципфа
было выполнено сравнение реальных частот токенов с теоретическими,
вычисленными по формуле:
\[
f(r) = \frac{C}{r},
\]
где $r$ — ранг токена, а $C$ — константа Ципфа.

В ходе эксперимента была получена константа
\[
C = 527\,771.
\]

Средняя абсолютная ошибка для первых 30 наиболее частотных слов
составила \textbf{11.7\%}, что свидетельствует о хорошем
соответствии распределения частот слов закону Ципфа.

\subsection{Визуализация}
\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{zipf_1.png}
\caption{Закон Ципфа}
\label{fig:zipf1}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.9\textwidth]{zipf_2.png}
\caption{500 самых частых терминов}
\label{fig:zipf2}
\end{figure}

\subsection{Стемминг и нормализация словоформ}

На данном этапе выполнялась морфологическая нормализация токенов
с использованием алгоритма стемминга.
Целью эксперимента являлась оценка влияния стемминга на размер словаря,
частотные характеристики корпуса и соответствие распределения
закону Ципфа.

\subsection{Сравнение токенизации без и со стеммингом}
Для анализа были выполнены два варианта обработки текста: токенизация без применения стемминга и токенизация с применением стемминга.

При этом общее количество токенов в обоих случаях оставалось неизменным,
что позволило корректно сравнить размеры словарей.

Результаты сравнения представлены ниже:
\begin{itemize}
    \item количество токенов — \textbf{6\,515\,040};
    \item уникальных токенов без стемминга — \textbf{126\,339};
    \item уникальных токенов со стеммингом — \textbf{87\,334}.
\end{itemize}

Применение стемминга позволило сократить размер словаря
на \textbf{39\,005 слов}, что соответствует уменьшению
на \textbf{30.9\%}.

\subsection{Влияние стемминга на частотное распределение}
Анализ наиболее частотных токенов показал,
что после применения стемминга различные словоформы
объединяются в общие основы.
В результате в числе наиболее частотных токенов
появляются укороченные основы слов,
такие как <<команд>>, <<машин>>, <<гонк>>, <<трасс>> и др.

Это приводит к перераспределению частот и увеличению
доли тематически значимых терминов,
характерных для новостного корпуса Формулы~1.

\subsection{Производительность алгоритма}
Сравнение времени выполнения показало,
что применение стемминга увеличивает вычислительные затраты:

\begin{itemize}
    \item токенизация без стемминга — \textbf{3.40 сек};
    \item токенизация со стеммингом — \textbf{5.44 сек}.
\end{itemize}

Таким образом, использование стемминга приводит
к замедлению обработки примерно на \textbf{60\%},
что является ожидаемым результатом
из-за дополнительной морфологической обработки каждого токена.

\subsection{Закон Ципфа для стеммированных токенов}
Для корпуса со стеммированными токенами
также была выполнена проверка закона Ципфа.
Полученная константа Ципфа составила:
\[
C = 544\,976.
\]

Средняя абсолютная ошибка аппроксимации
для первых 50 токенов составила \textbf{28.8\%}.
Несмотря на увеличение отклонений по сравнению
с вариантом без стемминга,
распределение частот в целом сохраняет
характер, близкий к закону Ципфа.


Применение стемминга позволило существенно сократить
размер словаря и объединить различные словоформы,
что является важным этапом при построении поисковых индексов.
При этом наблюдается рост вычислительных затрат
и увеличение отклонений от классического закона Ципфа,
что является допустимым и ожидаемым эффектом
для тематически ограниченных текстовых корпусов.


\newpage
\section{Лабораторная работа №6. Построение булева поискового индекса}

\subsection{Внутреннее представление документов}
После токенизации каждый документ представляется
последовательностью нормализованных термов,
полученных путём понижения капитализации и стемминга.
Для каждого терма фиксируется список идентификаторов документов,
в которых он встречается.

\subsection{Структура индекса}
Индекс состоит из двух основных частей:
\begin{itemize}
    \item \textbf{инвертированный индекс}, сопоставляющий каждому терму
    список документов (posting list);
    \item \textbf{прямой индекс}, содержащий метаданные документов:
    идентификатор, URL и количество термов в документе.
\end{itemize}

\subsection{Инвертированный индекс}
Инвертированный индекс реализован на основе хеш-таблицы
с разрешением коллизий методом цепочек.
Каждому терму соответствует posting-лист,
содержащий уникальные идентификаторы документов.
Перед записью в файл posting-листы сортируются
по возрастанию ID документов.

\subsection{Прямой индекс}
Прямой индекс хранит минимальный набор метаданных,
необходимый для формирования поисковой выдачи:
идентификатор документа, URL и общее число термов.
Данные располагаются последовательно в бинарном файле.

\subsection{Бинарный формат индекса}
Для хранения индекса был разработан собственный бинарный формат,
поддерживающий расширение в последующих лабораторных работах.

 Заголовок содержит сигнатуру файла, версию формата,
    количество термов и документов, а также смещения до основных секций.
 Инвертированный индекс хранится в виде
    упорядоченного списка термов с соответствующими posting-листами.
 Прямой индекс содержит последовательность записей документов.

Формат использует фиксированные типы данных
(\code{uint16}, \code{uint32}, \code{uint64}),
что обеспечивает компактность и переносимость.

\subsection{Метод сортировки}
Для сортировки термов и posting-листов
использовался алгоритм QuickSort.
Его достоинствами являются высокая практическая скорость,
работа на месте и хорошая локальность кэша.
К недостаткам относится нестабильность и худший случай
со сложностью $O(n^2)$, однако на реальных данных
данный эффект не проявляется.

\subsection{Результаты индексации}
В результате построения булева индекса были получены следующие показатели:
\begin{itemize}
    \item количество документов — \textbf{28\,264};
    \item количество уникальных термов — \textbf{87\,314};
    \item общее количество обработанных токенов — \textbf{6.5 млн};
    \item средняя длина терма — \textbf{9.07 символов}.
\end{itemize}

Средняя длина терма превышает среднюю длину токена,
полученную в предыдущих лабораторных работах,
что объясняется исключением коротких служебных слов
и доминированием тематически значимых терминов.

\subsection{Производительность}
Общее время построения индекса составило \textbf{136 секунд},
что соответствует:
\begin{itemize}
    \item \textbf{4.8 мс} на один документ;
    \item \textbf{466 КБ/сек} входных данных.
\end{itemize}

Производительность ограничена скоростью последовательного чтения данных,
сортировкой термов и операциями записи в файл.

\subsection{Масштабируемость и оптимизация}
При увеличении объёма входных данных в 10 раз
время индексации возрастёт линейно.
При увеличении в 100 и более раз
потребуются методы внешней сортировки,
буферизация ввода-вывода и распределённая индексация.

Для возможных направлений оптимизаций можно выбрать многопоточную обработку документов, использование \code{mmap} для работы с файлами, сжатие posting-листов (Gap encoding, VByte) и инкрементальное обновление индекса.

\newpage

\section{Лабораторная работа №7. Булев поиск}

\subsection{Поддерживаемый синтаксис запросов}
Реализованный поисковый движок поддерживает следующий синтаксис булевых запросов:
\begin{itemize}
    \item логическая операция \textbf{И}: пробел или оператор \texttt{\&\&};
    \item логическая операция \textbf{ИЛИ}: оператор \texttt{||};
    \item логическая операция \textbf{НЕ}: оператор \texttt{!};
    \item группировка выражений с помощью круглых скобок.
\end{itemize}

Парсер запросов устойчив к произвольному количеству пробелов и допускает нестрогое форматирование запроса пользователем.

Примеры поддерживаемых запросов: \guillemotleft чемпионат мира\guillemotright, \guillemotleft(красный || белая) машина\guillemotright, \guillemotleft руки !ноги\guillemotright.

\subsection{Архитектура поисковой системы}

Поисковая система работает поверх бинарного индекса, сформированного в ЛР6.  

Сначала загрузка бинарного индекса в оперативную память. Следом идет токенизация и нормализация поискового запроса. Затем разбор запроса с помощью рекурсивного нисходящего парсера. Далее выполнение булевых операций над постинг-листами и формирование и вывод поисковой выдачи.

Для повышения производительности все термы и постинг-листы полностью загружаются в память при старте поисковой системы.

\subsection{Разбор поисковых запросов}

Разбор поисковых запросов реализован с использованием рекурсивного спуска и следующей грамматики:
\begin{itemize}
    \item Expression ::= Term \{ OR Term \}
    \item Term ::= Factor \{ AND Factor \}
    \item Factor ::= NOT Factor | WORD | ( Expression )
\end{itemize}

Пробел между термами интерпретируется как логическая операция AND. При обработке запросов выполняется понижение капитализации и примитивный стемминг, что повышает полноту поиска.

\subsection{Булевы операции}

Для выполнения запросов реализованы следующие операции над отсортированными постинг-листами:
\begin{itemize}
    \item пересечение (AND) — линейное слияние двух списков;
    \item объединение (OR) — линейное объединение двух списков;
    \item отрицание (NOT) — формирование списка документов, не содержащих указанный термин.
\end{itemize}

Все операции выполняются за линейное время относительно длины обрабатываемых постинг-листов.

\subsection{Режимы работы}

Реализованы два режима работы поисковой системы:
\begin{enumerate}
    \item \textbf{Интерактивный режим}, позволяющий вводить поисковые запросы из стандартного ввода и получать поисковую выдачу с указанием времени выполнения запроса;
    \item \textbf{Пакетный режим}, предназначенный для обработки набора запросов из входного файла с сохранением результатов в выходной файл.
\end{enumerate}

\subsection{Поисковая выдача}

Поисковая выдача содержит общее количество найденных документов, и также идентификаторы документов и соответствующие им URL.

При превышении лимита выводится ссылка на получение следующей порции результатов.

\subsection{Пример работы программы}
\begin{figure}[H]
\centering
\includegraphics[width=0.7\textwidth]{query.png}
\caption{Пример выдачи}
\end{figure}

\subsection{Производительность поиска}

Для корпуса из 28\,264 документов и 87\,314 уникальных термов были получены следующие показатели:
\begin{itemize}
    \item время выполнения простых запросов (1--2 терма): менее 1 мс;
    \item время выполнения сложных запросов с несколькими скобками и операцией NOT: до нескольких миллисекунд;
    \item время загрузки индекса в память: доминирующая составляющая при старте программы.
\end{itemize}

Наиболее длительное время выполнения наблюдается для запросов с большим числом операций отрицания, так как операция NOT требует обхода всего множества документов.

\subsection{Тестирование корректности}

Корректность работы поисковой системы проверялась посредством ручной проверки результатов для простых запросов. Также производилось сравнение результатов запросов с логически эквивалентными выражениями. Проверка граничных случаев (пустой результат, запросы с отсутствующими термами) и тестирование устойчивости к некорректному синтаксису запросов.


\newpage
\section{Выводы}

В ходе лабораторных работ по курсу «Информационный поиск» был пройден полный цикл от анализа корпуса документов до построения поисковой системы с булевым поиском. Сначала проведён анализ корпуса новостных статей с сайтов \code{f1news.ru} и \code{f1report.ru}, что позволило оценить объём данных и эффективность очистки HTML-разметки. После обработки средний размер текста одного документа составил от 8 до 18 КБ. 

Далее разработан устойчивый поисковый робот, который собирал данные и сохранял их в реляционной базе PostgreSQL с контролем прогресса и проверкой изменений документов через хеширование. Обработка текста включала токенизацию, частотный анализ и стемминг. В корпусе из более чем 13 миллионов токенов насчитывалось 129 тысяч уникальных слов, применение стемминга сократило словарь до 87 тысяч терминов, объединяя различные словоформы. Распределение слов соответствовало закону Ципфа, средняя ошибка аппроксимации первых частотных слов составляла около 12–29\%, что указывает на хорошее соответствие теоретическим ожиданиям.

На основе подготовленного корпуса был построен булев поисковый индекс для 35 тысяч документов с 87 тысячами уникальных термов. Индекс позволил выполнять запросы с логическими операциями И, ИЛИ и НЕ с высокой скоростью: простые запросы обрабатывались за менее чем 1 мс, сложные — за несколько миллисекунд. Построенный индекс продемонстрировал эффективность хранения и поиска, а бинарный формат обеспечил компактность и удобство расширения системы.

Таким образом, проведённая работа охватывает все ключевые этапы информационного поиска: сбор и очистку данных, статистический анализ текста, морфологическую нормализацию, построение эффективного поискового индекса и реализацию полнофункциональной поисковой системы. Полученные результаты подтверждают правильность выбранных методов и готовность системы к дальнейшему развитию, включая расширение функционала поиска и внедрение ранжирования документов.

\newpage

\end{document}
